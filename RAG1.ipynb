{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cd6fda-10fd-4702-b24f-2ecde02dc0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.30.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (0.3.24)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (2.11.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-huggingface) (4.0.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-huggingface) (4.51.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-ollama langchain-community langchain-huggingface faiss-cpu huggingface_hub pandas python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe4a89e-679b-4df4-8aee-84827d842f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate  # Check if 'langchain_core' is valid or needs correction\n",
    "from langchain_ollama import OllamaLLM  # Corrected import for OllamaLLM\n",
    "from langchain.vectorstores import FAISS  # Correct import for FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Corrected import path\n",
    "import time\n",
    "import atexit\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define Ollama generation arguments\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "126f6cb3-d3a3-4115-bbcf-ad304f05433d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\acer\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: tenacity in c:\\users\\acer\\anaconda3\\lib\\site-packages (8.2.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Loaded 180142 records from hf://datasets/ncbi/Open-Patients/Open-Patients.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18015/18015 [00:00<00:00, 64844.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install remaining packages\n",
    "!pip install tqdm tenacity tiktoken\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import traceback\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type\n",
    "from huggingface_hub import login\n",
    "import tiktoken\n",
    "\n",
    "# Add a huggingface token to .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Use the Hugging Face access token for authentication\n",
    "hf_access_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# PATH_TO_DATA = \"hf://datasets/zhengyun21/PMC-Patients/PMC-Patients.csv\"\n",
    "PATH_TO_DATA = \"hf://datasets/ncbi/Open-Patients/Open-Patients.jsonl\"\n",
    "BATCH_SIZE = 10  # Adjust based on your needs\n",
    "PATH_TO_VECTORDB = \"./db/faiss_index\"\n",
    "TOKEN_LIMIT = 125  # Maximum number of tokens per chunk\n",
    "\n",
    "# Initialize tokenizer (assuming LLaMA tokenizer, you can replace with a specific tokenizer you use)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Replace with the correct tokenizer for your model if different\n",
    "\n",
    "# Create the vector DB directory if it doesn't exist\n",
    "if not os.path.exists(PATH_TO_VECTORDB):\n",
    "    os.makedirs(PATH_TO_VECTORDB)\n",
    "\n",
    "# Define retry decorator for API calls\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_random_exponential(min=1, max=60), \n",
    "       retry=retry_if_exception_type(Exception))\n",
    "def load_data_and_process():\n",
    "    try:\n",
    "        # Load the dataset (example with JSONL)\n",
    "        dataset = pd.read_json(PATH_TO_DATA, lines=True)\n",
    "        print(f\"Loaded {len(dataset)} records from {PATH_TO_DATA}\")\n",
    "        \n",
    "        # You can process your data here (e.g., tokenizing, embedding, etc.)\n",
    "        # For example, split data into batches and process\n",
    "        for i in tqdm(range(0, len(dataset), BATCH_SIZE)):\n",
    "            batch = dataset.iloc[i:i+BATCH_SIZE]\n",
    "            # Process each batch here (e.g., embeddings, vectorization, etc.)\n",
    "            # Using FAISS, HuggingFace embeddings, or your desired method.\n",
    "            \n",
    "        print(\"Data processed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Call the data processing function\n",
    "load_data_and_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bdc9ed-062f-46c1-ba56-b57d76036031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 03:11:24,217 - INFO - Vector database already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type\n",
    "import tiktoken\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Define constants\n",
    "PATH_TO_DATA = \"hf://datasets/ncbi/Open-Patients/Open-Patients.jsonl\"\n",
    "BATCH_SIZE = 10\n",
    "PATH_TO_VECTORDB = \"./db/faiss_index\"\n",
    "TOKEN_LIMIT = 125  # Maximum number of tokens per chunk\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Function to chunk text into smaller pieces\n",
    "def chunk_text(text, token_limit):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), token_limit):\n",
    "        chunk_tokens = tokens[i:i + token_limit]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# Function to create documents from rows\n",
    "def create_documents(row):\n",
    "    if not row['description']:  # Skip rows with empty descriptions\n",
    "        return []\n",
    "    chunks = chunk_text(row['description'], TOKEN_LIMIT)\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"_id\": row['_id']}\n",
    "        )\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "# Retry function for embedding\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6), retry=retry_if_exception_type(Exception))\n",
    "def embed_with_backoff(embeddings, texts):\n",
    "    return embeddings.embed_documents(texts)\n",
    "\n",
    "# Process each batch\n",
    "def process_batch(batch, embeddings):\n",
    "    try:\n",
    "        return embed_with_backoff(embeddings, batch)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error embedding batch: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main function to build the vector database\n",
    "def main():\n",
    "    if os.path.exists(PATH_TO_VECTORDB):\n",
    "        logger.info('Vector database already exists')\n",
    "    else:\n",
    "        try:\n",
    "            logger.info('Loading data')\n",
    "            df = pd.read_json(PATH_TO_DATA, lines=True)  # Assuming the JSON is in \"lines\" format\n",
    "            logger.info(f\"Loaded {len(df)} rows of data.\")\n",
    "\n",
    "            df = df.head(10)  # Limiting to first 10 rows for testing\n",
    "            total_rows = len(df)\n",
    "\n",
    "            logger.info('Creating documents')\n",
    "            documents = []\n",
    "            for _, row in tqdm(df.iterrows(), total=total_rows, desc=\"Creating documents\"):\n",
    "                documents.extend(create_documents(row))\n",
    "\n",
    "            logger.info('Building vector database...')\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\", model_kwargs={\"trust_remote_code\": True})\n",
    "\n",
    "            batches = [documents[i:i + BATCH_SIZE] for i in range(0, len(documents), BATCH_SIZE)]\n",
    "\n",
    "            all_embeddings = []\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [executor.submit(process_batch, [doc.page_content for doc in batch], embeddings) for batch in batches]\n",
    "                for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        all_embeddings.extend(result)\n",
    "\n",
    "            valid_docs = [doc for doc, emb in zip(documents, all_embeddings) if emb is not None]\n",
    "            valid_embeddings = [emb for emb in all_embeddings if emb is not None]\n",
    "\n",
    "            logger.info(f\"Successfully embedded {len(valid_docs)} out of {len(documents)} documents\")\n",
    "\n",
    "            vector_store = FAISS.from_embeddings(\n",
    "                text_embeddings=list(zip([doc.page_content for doc in valid_docs], valid_embeddings)),\n",
    "                embedding=embeddings,\n",
    "                metadatas=[doc.metadata for doc in valid_docs]\n",
    "            )\n",
    "\n",
    "            retriever = vector_store.as_retriever()\n",
    "\n",
    "            logger.info('Saving vector database locally...')\n",
    "            vector_store.save_local(PATH_TO_VECTORDB)\n",
    "\n",
    "            logger.info('Done')\n",
    "            return vector_store, retriever\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred: {str(e)}\")\n",
    "            logger.error(\"Traceback:\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return \"Error occurred\", \"Error occurred\", [], []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30443abc-7f69-4b4c-82e1-f63dd779d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Ollama model once (reuse across calls)\n",
    "model = OllamaLLM(model=\"llama3.2:3b\")\n",
    "\n",
    "def query_ollama(prompt, generation_kwargs, output_format=\"text\"):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Ollama model using langchain-ollama integration.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The generated prompt to send to the model.\n",
    "        generation_kwargs (dict): Generation settings for the model.\n",
    "        output_format (str): The expected format of the output, either \"text\" or \"json\".\n",
    "    \n",
    "    Returns:\n",
    "        str or dict: The model's response, either as text or a parsed JSON object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create prompt template\n",
    "        chat_prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        chain = chat_prompt | model\n",
    "        \n",
    "        # Invoke the chain with the prompt\n",
    "        response = chain.invoke({\"question\": prompt})\n",
    "        \n",
    "        # Check output format\n",
    "        if output_format == \"json\":\n",
    "            try:\n",
    "                result = json.loads(response)\n",
    "                return result\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(f\"Failed to decode JSON response: {response}\")\n",
    "                return {\n",
    "                    \"error\": \"Error in generating response or unexpected response format.\"\n",
    "                }\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the Ollama query: {str(e)}\")\n",
    "        return {\"error\": \"An error occurred during the query.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4a6dba-dae3-4ab2-a11b-7cb20f976bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article_text, query_ollama, generation_kwargs):\n",
    "    \"\"\"\n",
    "    Generates a concise summary of the article text, focusing on relevant phenotypes, diagnoses, \n",
    "    and other attributes of the patient.\n",
    "\n",
    "    Args:\n",
    "    - article_text (str): The full text of the article that needs to be summarized.\n",
    "    - query_ollama (function): The function to query the Ollama model.\n",
    "    - generation_kwargs (dict): Additional generation arguments for the model.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The summary of the article text.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following patient case in a single paragraph, focusing on relevant phenotypes, diagnoses, and other attributes of the patient.\n",
    "    The summary should be concise and retain important scientific or clinical terminology.\n",
    "    ### Article Text:\n",
    "    {article_text}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    response = query_ollama(prompt, generation_kwargs, output_format=\"text\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_potential_causes(patient_case, article_summaries, query_ollama, generation_kwargs):\n",
    "    \"\"\"\n",
    "    Generates a prompt to query Ollama for potential causes based on a patient's case and article summaries.\n",
    "\n",
    "    Args:\n",
    "    - patient_case (str): The user's provided detailed patient case.\n",
    "    - article_summaries (list): List of summaries from similar patient cases in the literature.\n",
    "    - query_ollama (function): The function to query the Ollama model.\n",
    "    - generation_kwargs (dict): Additional generation arguments for the model.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated prompt to query Ollama.\n",
    "    \"\"\"\n",
    "    examples = \"\\n\\n\".join([f\"### Example Case {i+1}:\\n{summary}\" for i, summary in enumerate(article_summaries)])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    A user has provided a detailed patient's case with specific symptoms and medical history.\n",
    "    Below are some examples of similar patient cases from medical literature.\n",
    "\n",
    "    Your task is to identify potential causes for the symptoms described in the user's case, using the examples as references.\n",
    "\n",
    "    ### User-Provided Patient Case:\n",
    "    {patient_case}\n",
    "\n",
    "    {examples}\n",
    "\n",
    "    Please provide:\n",
    "    - A numbered list of potential causes based on the information.\n",
    "    - A brief explanation for each cause.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query Ollama for potential causes\n",
    "    response = query_ollama(prompt, generation_kwargs, output_format=\"text\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "765cdf03-c591-43bc-9d84-6c2c6d859151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 03:17:06,639 - INFO - Attempt 1/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:06,641 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 1/10...\n",
      "2025-04-08 03:17:08,644 - INFO - Attempt 2/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:08,646 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 2/10...\n",
      "2025-04-08 03:17:10,648 - INFO - Attempt 3/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:10,649 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 3/10...\n",
      "2025-04-08 03:17:12,652 - INFO - Attempt 4/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:12,652 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 4/10...\n",
      "2025-04-08 03:17:14,656 - INFO - Attempt 5/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:14,657 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 5/10...\n",
      "2025-04-08 03:17:16,660 - INFO - Attempt 6/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:16,661 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 6/10...\n",
      "2025-04-08 03:17:18,663 - INFO - Attempt 7/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:18,663 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 7/10...\n",
      "2025-04-08 03:17:20,667 - INFO - Attempt 8/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:20,669 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 8/10...\n",
      "2025-04-08 03:17:22,671 - INFO - Attempt 9/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:22,671 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 9/10...\n",
      "2025-04-08 03:17:24,675 - INFO - Attempt 10/10 - Processing Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2025-04-08 03:17:24,675 - ERROR - Error occurred: name 'vector_store' is not defined. Retrying 10/10...\n",
      "2025-04-08 03:17:26,678 - WARNING - Max retries reached, could not complete the request.\n",
      "2025-04-08 03:17:26,680 - INFO - Potential causes identified and saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def ask_me_potential_causes(patient_narrative, max_retries=10, delay=2, k=4):\n",
    "    \"\"\"\n",
    "    Asks for potential causes of a patient's condition based on a provided narrative and retrieves similar cases.\n",
    "    \n",
    "    Args:\n",
    "    - patient_narrative (str): A detailed description of the patient's case.\n",
    "    - max_retries (int): The maximum number of retry attempts in case of failure.\n",
    "    - delay (int): The time delay (in seconds) between retry attempts.\n",
    "    - k (int): The number of similar cases to retrieve from the vector store.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The potential causes for the patient case, or a failure message if retries are exceeded.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            logger.info(f\"Attempt {retries+1}/{max_retries} - Processing Patient Case: {patient_narrative}\")\n",
    "            \n",
    "            # Retrieve similar cases from FAISS vector store\n",
    "            docs = vector_store.similarity_search_with_score(patient_narrative, k=k)\n",
    "            article_summaries = [doc.page_content for doc, score in docs]\n",
    "\n",
    "            # Check if summaries are retrieved successfully\n",
    "            if not article_summaries:\n",
    "                logger.error(\"No similar cases found.\")\n",
    "                return \"No similar cases found.\"\n",
    "\n",
    "            # Save related cases in human-readable format to a .txt file\n",
    "            output_path_cases_txt = \"related_patient_cases.txt\"\n",
    "            try:\n",
    "                with open(output_path_cases_txt, 'w') as txt_file:\n",
    "                    txt_file.write(\"Related Patient Cases (Double-Spaced):\\n\\n\")\n",
    "                    for i, summary in enumerate(article_summaries):\n",
    "                        txt_file.write(f\"Case {i+1}:\\n{summary}\\n\\n\")\n",
    "                logger.info(f\"Related patient cases saved to {output_path_cases_txt} in human-readable format.\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"Error writing related cases to file: {e}\")\n",
    "                return \"Error saving related patient cases.\"\n",
    "\n",
    "            # Generate a prompt to find potential causes\n",
    "            prompt = generate_potential_causes(patient_narrative, article_summaries)\n",
    "            potential_causes = query_ollama(prompt, generation_kwargs, output_format=\"text\")\n",
    "\n",
    "            # Check if potential causes were generated\n",
    "            if not potential_causes:\n",
    "                logger.error(\"No potential causes generated.\")\n",
    "                return \"No potential causes generated.\"\n",
    "\n",
    "            # Save potential causes to a .txt file\n",
    "            output_path_causes_txt = \"potential_causes.txt\"\n",
    "            try:\n",
    "                with open(output_path_causes_txt, 'w') as txt_file:\n",
    "                    txt_file.write(\"Potential Causes for Patient Case:\\n\\n\")\n",
    "                    txt_file.write(potential_causes)\n",
    "                logger.info(f\"Potential causes saved to {output_path_causes_txt}.\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"Error writing potential causes to file: {e}\")\n",
    "                return \"Error saving potential causes.\"\n",
    "\n",
    "            return potential_causes\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            logger.error(f\"Error occurred: {e}. Retrying {retries}/{max_retries}...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    logger.warning(\"Max retries reached, could not complete the request.\")\n",
    "    return \"Max retries reached, could not complete the request.\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    patient_narrative = \"64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\"\n",
    "    result = ask_me_potential_causes(patient_narrative)\n",
    "    logger.info(\"Potential causes identified and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc88054-261d-4d2b-b538-d66d384faea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
